{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yThcx993g2fF"
   },
   "source": [
    "# 予測関数の実装\n",
    "MNISTの学習済み重みを用いて、予測(クラス分類を行うこと)関数を実装していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QWcCfY6riJRU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ローカル環境での実行です\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    print('Google Colab. 上での実行です')\n",
    "    print('「ファイルを選択」から、DAY1/2_notebookフォルダのsample_weight.pklを選択し、アップロードしてください')\n",
    "    print('===========')\n",
    "    files.upload()\n",
    "except:\n",
    "    print('ローカル環境での実行です')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:18:37.183381Z",
     "start_time": "2018-03-30T07:18:36.094825Z"
    },
    "id": "tF7OG_9cg2fH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 03:35:27.199470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-07 03:35:27.444235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-07 03:35:27.444273: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-07 03:35:27.478796: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-07 03:35:28.274687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-07 03:35:28.274836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-07 03:35:28.274850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "train = X_train/255\n",
    "test = X_test/255\n",
    "train = train.reshape(-1, 28*28)\n",
    "test = test.reshape(-1, 28*28)\n",
    "train_labels = lb.fit_transform(y_train)\n",
    "test_labels = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "057MstcQg2fJ"
   },
   "source": [
    "## 学習済みの重みを読む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:18:37.214758Z",
     "start_time": "2018-03-30T07:18:37.188209Z"
    },
    "id": "ggItm-q5g2fJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['b2', 'W1', 'b1', 'W2', 'W3', 'b3'])\n",
      "\n",
      "[[-0.00741249 -0.00790439 -0.01307499 ...  0.01978721 -0.04331266\n",
      "  -0.01350104]\n",
      " [-0.01029745 -0.01616653 -0.01228376 ...  0.01920228  0.02809811\n",
      "   0.01450908]\n",
      " [-0.01309184 -0.00244747 -0.0177224  ...  0.00944778  0.01387301\n",
      "   0.03393568]\n",
      " ...\n",
      " [ 0.02242565 -0.0296145  -0.06326169 ... -0.01012643  0.01120969\n",
      "   0.01027199]\n",
      " [-0.00761533  0.02028973 -0.01498873 ...  0.02735376 -0.01229855\n",
      "   0.02407041]\n",
      " [ 0.00027915 -0.06848375  0.00911191 ... -0.03183098  0.00743086\n",
      "  -0.04021148]]\n"
     ]
    }
   ],
   "source": [
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "network = init_network()\n",
    "print(network.keys())\n",
    "print(\"\")\n",
    "\n",
    "W1, W2, W3 = network[\"W1\"],network[\"W2\"],network[\"W3\"]\n",
    "b1, b2, b3 = network[\"b1\"],network[\"b2\"],network[\"b3\"] \n",
    "\n",
    "print(network[\"W1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSqQWu7Eg2fK"
   },
   "source": [
    "### 行列の形状の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:18:37.227838Z",
     "start_time": "2018-03-30T07:18:37.217938Z"
    },
    "id": "T9E86fIYg2fK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50) (50, 100) (100, 10)\n",
      "             (50,)        (100,)      (10,)\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape, W2.shape, W3.shape)\n",
    "print(\"            \" , b1.shape, \"      \" ,b2.shape,\"    \" ,  b3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:12:40.356863Z",
     "start_time": "2018-03-30T06:12:40.349220Z"
    },
    "id": "6GHUS2Hlg2fK"
   },
   "source": [
    "### [問]\n",
    "* 入力層のノード数は？\n",
    "    * 784個・・・y1=x1*W1+b1における[x1の要素数]\n",
    "* 1つ目の中間層のノード数は？\n",
    "    * 50個・・・b1の要素数\n",
    "* 2つ目の中間層のノード数は？\n",
    "    * 100個・・・b2の要素数\n",
    "* 出力層のノード数は？\n",
    "    * 10個・・・b3の要素数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "823hekt9g2fK"
   },
   "source": [
    "## 予測関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB624Xnwg2fL"
   },
   "source": [
    "### [演習]\n",
    "* 以下のpredict関数を完成させましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:18:37.275481Z",
     "start_time": "2018-03-30T07:18:37.231652Z"
    },
    "id": "xXC70ADvg2fL"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+ np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    c = np.max(x)    \n",
    "    exp_a = np.exp(x - c)\n",
    "    return exp_a / np.sum(exp_a)\n",
    "\n",
    "def predict(network, x):\n",
    "    \"\"\"\n",
    "    クラス分類するための関数\n",
    "    network : 重み行列を納めたdict\n",
    "    x : 入力ベクトル\n",
    "    return : ソフトマックス関数の結果\n",
    "    \"\"\"\n",
    "    W1, W2, W3 = network[\"W1\"],network[\"W2\"],network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"],network[\"b2\"],network[\"b3\"]    \n",
    "    \n",
    "    # 順伝播の計算\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEXfbRIqg2fL"
   },
   "source": [
    "### 1データずつ予測を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:18:37.400017Z",
     "start_time": "2018-03-30T07:18:37.278436Z"
    },
    "id": "44fgxAeag2fM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9\n",
      " 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8\n",
      " 7 3 9 7 4 4 4 9 2 5 4 7 6 7 9 0 5 8 5 6 6 5 7 8 1 0 1 6 4 6 7 3 1 7 1 8 2\n",
      " 0 2 9 9 5 5 1 5 6 0 3 4 4 6 5 4 6 5 4 5 1 4 4 7 2 3 2 7 1 8 1 8 1 8 5 0 8\n",
      " 9 2 5 0 1 1 1 0 9 0 3 1 6 4 2 3 6 1 1 1 3 9 5 2 9 4 5 9 3 9 0 3 6 5 5 7 2\n",
      " 2 7 1 2 8 4 1 7 3 3 8 8 7 9 2 2 4 1 5 9 8 7 2 3 0 4 4 2 4 1 9 5 7 7 2 8 2\n",
      " 6 8 5 7 7 9 1 8 1 8 0 3 0 1 9 9 4 1 8 2 1 2 9 7 5 9 2 6 4 1 5 8 2 9 2 0 4\n",
      " 0 0 2 8 4 7 1 2 4 0 2 7 4 3 3 0 0 3 1 9 6 5 2 5 9 2 9 3 0 4 2 0 7 1 1 2 1\n",
      " 5 3 3 9 7 8 6 5 6 1 3 8 1 0 5 1 3 1 5 5 6 1 8 5 1 7 9 4 6 2 2 5 0 6 5 6 3\n",
      " 7 2 0 8 8 5 4 1 1 4 0 3 3 7 6 1 6 2 1 9 2 8 6 1 9 5 2 5 4 4 2 8 3 8 2 4 5\n",
      " 0 3 1 7 7 5 7 9 7 1 9 2 1 4 2 9 2 0 4 9 1 4 8 1 8 4 5 9 8 8 3 7 6 0 0 3 0\n",
      " 2 6 6 4 9 3 3 3 2 3 9 1 2 6 8 0 5 6 6 6 3 8 8 2 7 5 8 9 6 1 8 4 1 2 5 9 1\n",
      " 9 7 5 4 0 8 9 9 1 0 5 2 3 7 8 9 4 0 6 3 9 5 2 1 3 1 3 6 5 7 4 2 2 6 3 2 6\n",
      " 5 4 8 9 7 1 3 0 3 8 3 1 9 3 4 4 6 4 2 1 8 2 5 4 8 8 4 0 0 2 3 2 7 7 0 8 7\n",
      " 4 4 7 9 6 9 0 9 8 0 4 6 0 6 3 5 4 8 3 3 9 3 3 3 7 8 0 8 2 1 7 0 6 5 4 3 8\n",
      " 0 9 6 3 8 0 9 9 6 8 6 8 5 7 8 6 0 2 4 0 2 2 3 1 9 7 5 1 0 8 4 6 2 6 7 9 3\n",
      " 2 9 8 2 2 9 2 7 3 5 9 1 8 0 2 0 5 2 1 3 7 6 7 1 2 5 8 0 3 7 2 4 0 9 1 8 6\n",
      " 7 7 4 3 4 9 1 9 5 1 7 3 9 7 6 9 1 3 7 8 3 3 6 7 2 8 5 8 5 1 1 4 4 3 1 0 7\n",
      " 7 0 7 9 4 4 8 5 5 4 0 8 2 1 0 8 4 5 0 4 0 6 1 7 3 2 6 7 2 6 9 3 1 4 6 2 5\n",
      " 4 2 0 6 2 1 7 3 4 1 0 5 4 3 1 1 7 4 9 9 4 8 4 0 2 4 5 1 1 6 4 7 1 9 4 2 4\n",
      " 1 5 5 3 8 3 1 4 5 6 8 9 4 1 5 3 8 0 3 2 5 1 2 8 3 4 4 0 8 8 3 3 1 7 3 5 9\n",
      " 6 3 2 6 1 3 6 0 7 2 1 7 1 4 2 4 2 1 7 9 6 1 1 2 4 8 1 7 7 4 8 0 7 3 1 3 1\n",
      " 0 7 7 0 3 5 5 2 7 6 6 9 2 8 3 5 2 2 5 6 0 8 2 9 2 8 8 8 8 7 4 9 3 0 6 6 3\n",
      " 2 1 3 2 2 9 3 0 0 5 7 8 1 4 4 6 0 2 9 1 4 7 4 7 3 9 8 8 4 7 1 2 1 2 2 3 2\n",
      " 3 2 3 9 1 7 4 0 3 5 5 8 6 3 2 6 7 6 6 3 2 7 8 1 1 7 5 6 4 9 5 1 3 3 4 7 8\n",
      " 9 1 1 6 9 1 4 4 5 4 0 6 2 2 3 1 5 1 2 0 3 8 1 2 6 7 1 6 2 3 9 0 1 2 2 0 8\n",
      " 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.085"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストデータについて、識別を行う\n",
    "t = np.argmax(test_labels, axis=1)\n",
    "p = np.array([]).astype(int)\n",
    "NUM = 1000\n",
    "for i in range(NUM):\n",
    "    y = predict(network, test[i])\n",
    "    p = np.append(p, np.argmax(y))\n",
    "    \n",
    "#　正解データ    \n",
    "t = np.argmax(test_labels[:NUM], axis=1)\n",
    "\n",
    "# 予測結果と正解データの比較\n",
    "print(p)\n",
    "print(t)\n",
    "\n",
    "# 識別精度\n",
    "accuracy_score(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_7_predict_trainee.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
