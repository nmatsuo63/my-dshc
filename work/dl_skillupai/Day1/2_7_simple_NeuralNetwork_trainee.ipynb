{"cells":[{"cell_type":"markdown","metadata":{"id":"J6FrxCDtI96_"},"source":["# ニューラルネットワークにおいて勾配を求めるクラス"]},{"cell_type":"code","source":["import numpy as np\n","\n","try:\n","    from google.colab import files\n","    print('Google Colab. 上での実行です')\n","    print('「ファイルを選択」から、notebook/commonフォルダのactivations.py、grad.py、loss.pyを選択し、アップロードしてください')\n","    print('===========')\n","    files.upload()\n","    !mkdir common\n","    !mv activations.py ./common\n","    !mv grad.py ./common\n","    !mv loss.py ./common\n","except:\n","    print('ローカル環境での実行です')\n","\n","from common.activations import softmax\n","from common.grad import numerical_gradient\n","from common.loss import cross_entropy_error"],"metadata":{"id":"Xmhq6oxaI-qS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkgW-8cAI97I"},"source":["* 「ニューラルネットワークにおいて勾配を求める」とは、損失$L$の$W$に対する偏微分 $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$を求めるということである。  \n","    例、  \n","  $${\\bf W}=\\begin{pmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{pmatrix}$$  \n","  \n","      \n","  $$\\displaystyle \n","  \\frac{\\partial L}{\\partial {\\bf W}}=\\begin{pmatrix} \n","  \\frac{\\partial L}{\\partial {w_{11}}} &\n","                                                                                  \\frac{\\partial L}{\\partial {w_{12}}} &\n","  \\frac{\\partial L}{\\partial {w_{13}}} \\\\\n","  \\frac{\\partial L}{\\partial {w_{21}}} &\n","  \\frac{\\partial L}{\\partial {w_{22}}} &\n","  \\frac{\\partial L}{\\partial {w_{23}}}\n","  \\end{pmatrix}$$  \n","                                                                                            \n","  \n","    \n","* $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$は、${\\bf W}$のそれぞれの要素が微小量変化したときの$L$の変化量を表す。\n","\n","\n","### [演習]\n","* シンプルなニューラルネットワークにおいて、予測、損失、勾配を計算する以下のクラスを完成させましょう\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtdOoNSLI97J"},"outputs":[],"source":["# ヒント\n","np.random.seed(seed=1234)\n","print(np.random.randn(2,3)) # 標準正規分布からランダムにサンプリング"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4GzU_ijI97K"},"outputs":[],"source":["class simpleNet:\n","    def __init__(self, seed=1234):\n","        np.random.seed(seed)\n","        self.W = np.random.randn(2,3)#Wの初期値\n","        \n","    def predict(self, x):\n","        \"\"\"\n","        予測関数\n","        \"\"\"\n","        z = np.dot(        ,      )\n","        y = softmax(       )\n","        return y\n","    \n","    def loss(self, x, t):\n","        \"\"\"\n","        損失関数\n","        \"\"\"\n","        y = self.predict(      )\n","        loss = cross_entropy_error(     ,      )\n","        return loss\n","    \n","    def gradient(self, x, t):\n","        \"\"\"\n","        勾配計算関数\n","        \"\"\"\n","        grads={}\n","        f = self.loss\n","        W = self.W\n","        grads[\"W\"] = numerical_gradient(        ,      ,      ,       )\n","\n","        return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"PD3sTtr2I97L"},"outputs":[],"source":["# 重みの初期値の確認\n","snet = simpleNet()        \n","print(\"net.W=\")\n","print(snet.W)\n","print()\n","\n","# 予測関数の確認\n","x = np.array([0.6, 0.9])\n","y = snet.predict(x)\n","print(\"y=\", y)\n","print()\n","print(\"np.argmax(y)=\", np.argmax(y))\n","print()\n","\n","# 損失を求める関数の確認\n","t =np.array([0, 0, 1]) #正解ラベル\n","print(\"loss=\",snet.loss(x, t))\n","print()\n","\n","# 勾配を求める\n","grad = snet.gradient(x, t)\n","print(\"grad=\")\n","print(grad)"]},{"cell_type":"markdown","metadata":{"id":"PR-RSyNLI97M"},"source":["### [gradの解釈]\n","* 求められたgradは、例えば、次のように解釈できる\n","  * $w_{11}$を正の方向に微小量$h$だけ変化させたとき、$L$は0.09増える。\n","  * $w_{13}$を負の方向に微小量$h$だけ変化させたとき、$L$は0.11増える。\n","  * $L$は$0$に近づけたいので、$w_{11}$は負の方向に更新し、$w_{13}$は正の方向に更新するのが良い、となる。"]},{"cell_type":"markdown","metadata":{"id":"g2ZqJRNoI97N"},"source":["### [演習]\n","* 重みWの初期値を変えてみましょう。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"2_7_simple_NeuralNetwork_trainee.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}