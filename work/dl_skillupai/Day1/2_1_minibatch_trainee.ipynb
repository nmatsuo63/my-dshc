{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチ学習の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T08:22:18.778283Z",
     "start_time": "2018-03-30T08:22:17.521441Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 04:31:33.361189: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-07 04:31:33.543886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-07 04:31:33.543987: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-07 04:31:33.617846: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-07 04:31:34.375333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-07 04:31:34.375633: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-07 04:31:34.375659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Load the MNIST dataset\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "train = X_train/255\n",
    "test = X_test/255\n",
    "train = train.reshape(-1, 28*28)\n",
    "test = test.reshape(-1, 28*28)\n",
    "train_labels = lb.fit_transform(y_train)\n",
    "test_labels = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "0 255\n"
     ]
    }
   ],
   "source": [
    "# 255で割る理由＝正規化（最小：0、最大：1への変換）\n",
    "# X_trainは60000個の画像データであり、各画像においては784（=28*28）個のセルの中に0〜255の数字が入っている\n",
    "print(X_train.shape)\n",
    "print(X_train.min(), X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習\n",
    "* ミニバッチ学習は、一般的には非復元抽出によって行われることが多いが、必ずこうしなければならないというわけではなく、分析者がデータセットの与え方を工夫することもできる。ただし、工夫しても計算が上手くいくとは限らない。\n",
    "* 工夫のしどころ。\n",
    "    * 一般的には、エポックのたびにシャッフルするが、シャッフルするタイミングを任意に変えてみる  \n",
    "    * 与えるミニバッチ の順番を意図的に操作してみる   \n",
    "        * 例、出現頻度の少ないラベルのデータを先に学習させる\n",
    "    * 抽出されるラベルの割合が一定になるように抽出してみる\n",
    "    * 復元抽出にしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T08:23:46.756032Z",
     "start_time": "2018-03-30T08:23:46.750904Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainer(network, x, y):\n",
    "    \"\"\"\n",
    "    学習用の関数\n",
    "    このnotebookでは、ミニバッチ学習を学ぶことが目的であるため、この関数の中身は空のままにしておく\n",
    "    実際には、何らかの学習させるための処理を記述する\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T08:24:31.841612Z",
     "start_time": "2018-03-30T08:24:31.837897Z"
    }
   },
   "source": [
    "### ミニバッチ学習のループ(復元抽出)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T08:25:54.693563Z",
     "start_time": "2018-03-30T08:25:54.673160Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0,  batch_mask=[27439 58067 34086 56373 23924 17048 55289 32399 55985 41239]\n",
      "i=1,  batch_mask=[22267 21580 14629 12198 50682 46275 10983 23691 39552 21225]\n",
      "i=2,  batch_mask=[29280 27532 28869 31741 49777  3039  8165 28319  8953 29692]\n",
      "i=3,  batch_mask=[50926 18991  3062 43252 43382 58031 58255 51746    10  3356]\n",
      "i=4,  batch_mask=[48917 55492 14455 34791 59642 43444 43456 20949  4468 43881]\n",
      "i=5,  batch_mask=[12184 57310 45801 55823  6984  4994 52624   619 21634 19186]\n",
      "i=6,  batch_mask=[27117 42059  4111 34580  4880 36288 26464 32382 26835 40249]\n",
      "i=7,  batch_mask=[ 9624 10252 10700 18272  7688 13615 12057 51949 55061 35990]\n",
      "i=8,  batch_mask=[49068 54819 35754 49556 43802 12633 59499 36759 32386 47848]\n",
      "i=9,  batch_mask=[ 9532 31315 31088 29429 21129 37436 32946 35249 59498 17095]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "train_size = train_labels.shape[0]\n",
    "batch_size = 32\n",
    "max_iter = 10  #ループの回数\n",
    "network = None #ダミー\n",
    "\n",
    "for i in range(max_iter):\n",
    "    # 復元抽出 train_size Combination batch_sizeのイメージ\n",
    "    batch_mask = np.random.choice(train_size, batch_size) \n",
    "    # バッチサイズは32としているが、ここではスペース省略のため10個のみ表示\n",
    "    print(\"i=%s, \"%i, \"batch_mask=%s\"%batch_mask[:10])\n",
    "    # numpyではtrain[batch_mask]とすることで、batch_maskの要素番目のものを取り出してくることができる\n",
    "    x_batch = train[batch_mask]\n",
    "    y_batch = train_labels[batch_mask]\n",
    "\n",
    "    # ここでは学習はしないので、ダミーのNetworkを与えている\n",
    "    trainer(network, x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 復元抽出部分を理解するためのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_mask= [27439 58067 34086 56373 23924 17048 55289 32399 55985 41239  9449 23706\n",
      "  8222 32427 33950 40684  8060  7962 13686 59834 59512 14192  7644 27973\n",
      " 27984 41929 51583 49398  2558 36271 38450  3824]\n",
      "\n",
      "x_batch= [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "x_batch.shape= (32, 784)\n",
      "\n",
      "y_batch= [[0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]]\n",
      "y_batch.shape= (32, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "# 32件のデータを取り出す\n",
    "print(\"batch_mask=\", batch_mask)\n",
    "print()\n",
    "\n",
    "x_batch = train[batch_mask]\n",
    "print(\"x_batch=\", x_batch)\n",
    "# ３２件のデータを取り出す。それぞれ784個の要素で構成されるデータである\n",
    "print(\"x_batch.shape=\", x_batch.shape)\n",
    "print()\n",
    "y_batch = train_labels[batch_mask]\n",
    "print(\"y_batch=\", y_batch)\n",
    "# 32件のデータを取り出す。それぞれ10個の要素で構成されるデータである\n",
    "# ここでは0, 1, ..., 9のどの数字を指しているかを示す\n",
    "print(\"y_batch.shape=\", y_batch.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 復元抽出部分(何回か実行してみてください)\n",
    "np.random.choice(10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ学習のループ(非復元抽出)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下の非復元抽出によるミニバッチ学習を完成させましょう。\n",
    "* 通常の計算では、非復元抽出で行うことが多いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=[0 1 2 3 4 5 6 7 8 9]\n",
      "index=[7 2 9 1 0 8 4 5 6 3]\n",
      "\n",
      "[7 3 5 1 4 8 0 2 6 9]\n",
      "\n",
      "[7 2 9]\n",
      "[1 0 8]\n",
      "[4 5 6]\n",
      "[3]\n",
      "2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# ヒント\n",
    "index = np.arange(10)\n",
    "print(\"index=%s\"%index)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(index)\n",
    "print(\"index=%s\"%index)\n",
    "print()\n",
    "print(np.random.permutation(10))\n",
    "print()\n",
    "\n",
    "for i in range(4):\n",
    "    print(index[3*i:3*(i+1)])\n",
    "    \n",
    "print(np.ceil(1.1), np.ceil(1.7), np.ceil(2.7)) # ceilは切り上げ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=0,  mn=0,  batch_mask=[30329 44957 30866 40447 25580  6216 26373  9010 23445   108]\n",
      "epoch=0,  mn=100,  batch_mask=[13196 36161 19297 47668 43204 53789 20787 28580 15708 28730]\n",
      "epoch=0,  mn=200,  batch_mask=[57861 11320 48581 10823   122  3304 36073 51240 52366 26261]\n",
      "epoch=0,  mn=300,  batch_mask=[30832 12049 14714  8507 14579 44748 46360 51998 31112 20184]\n",
      "epoch=0,  mn=400,  batch_mask=[55463 58348 47584  5632 34691  5580 14161 31994 24594 51992]\n",
      "epoch=0,  mn=500,  batch_mask=[13286 42771 30701 36253 20841 18060 26171 16097 53271  2561]\n",
      "epoch=0,  mn=600,  batch_mask=[17394 29041 56149 55570 19332 40836 45961 14389 28184 35640]\n",
      "epoch=0,  mn=700,  batch_mask=[29468  4704 57628 25574 47253 57210 48338 55438  9637  7583]\n",
      "epoch=0,  mn=800,  batch_mask=[  541 14048  7522 39030 20557  5302 57769 56218 57342  9972]\n",
      "epoch=0,  mn=900,  batch_mask=[36523 49833 41420 58800 50758 34552 55193 44471  3732 58420]\n",
      "epoch=0,  mn=1000,  batch_mask=[32066  2545 23122 10582 54259 39464 34687 23687 28855 29786]\n",
      "epoch=0,  mn=1100,  batch_mask=[17253 14210 18496 25196 29672 37857  3017  5104 28414 47410]\n",
      "epoch=0,  mn=1200,  batch_mask=[50735 56951 28466 28253 53035 39106 55461 28788 20068 48293]\n",
      "epoch=0,  mn=1300,  batch_mask=[ 6586 54852  6233 14816 43041  3762  5655  4408 10491  2457]\n",
      "epoch=0,  mn=1400,  batch_mask=[33773 41008 54970 24112 53080 48935 36802 31895 47898 12482]\n",
      "epoch=0,  mn=1500,  batch_mask=[20500 40501 26872  8871 20028 10412 26721 31119 17572  4135]\n",
      "epoch=0,  mn=1600,  batch_mask=[41747 37666 40419 44167 21681 15612 20045 48122 17279 16453]\n",
      "epoch=0,  mn=1700,  batch_mask=[21390 52969 24496 34116 12267 35872 20012 49896  9644  5280]\n",
      "epoch=0,  mn=1800,  batch_mask=[47146 26664 58784 26178 18333 26206 51134 48407 15546 59571]\n",
      "\n",
      "epoch=1,  mn=0,  batch_mask=[18318 45986  8813 18069  8351 20286 52477 22446 58545 31116]\n",
      "epoch=1,  mn=100,  batch_mask=[28819 18780 56044 27034 25960 26020 24943 26038 15996 24263]\n",
      "epoch=1,  mn=200,  batch_mask=[31003 26246 45402 13540 46986  1856 33653 23219 47086 18210]\n",
      "epoch=1,  mn=300,  batch_mask=[21132 55581 58798 22876 48908 36800 43252  3938 36949  4134]\n",
      "epoch=1,  mn=400,  batch_mask=[11592 55570 34311 42990 48316 27234  8844 54778 24859 42936]\n",
      "epoch=1,  mn=500,  batch_mask=[11529 53148 52419 33433 52341 45784 25310 29596 20386 23512]\n",
      "epoch=1,  mn=600,  batch_mask=[ 8587 52560 57524  5409 45984 22293 12182 27844 49559 59237]\n",
      "epoch=1,  mn=700,  batch_mask=[11864 14402 47652  2809 38622 35764 44286 44412 50183  2778]\n",
      "epoch=1,  mn=800,  batch_mask=[ 7443 25316 26979 42003 28682 38893 51395 23051 30328 23835]\n",
      "epoch=1,  mn=900,  batch_mask=[ 4395 21872 21740 17978 53411 52669 43552 13116  1028 48911]\n",
      "epoch=1,  mn=1000,  batch_mask=[48994 23510 44409 30075  3361 18673 38400 36810 20262 35549]\n",
      "epoch=1,  mn=1100,  batch_mask=[ 2955 45175 52519 22854 53372 57012 56556 31897 59138 48505]\n",
      "epoch=1,  mn=1200,  batch_mask=[ 4035 59150 30647 48304 54104 44524  8677 52938 11486 25266]\n",
      "epoch=1,  mn=1300,  batch_mask=[53943  7214 15340 22359 53297  9999 46758 12150 42090   835]\n",
      "epoch=1,  mn=1400,  batch_mask=[ 6755  8144 41714 55436 37365 13644 39330 54912 19013 52972]\n",
      "epoch=1,  mn=1500,  batch_mask=[14851 28148 57421 34900 23597  7595  5810 27540 20950 15863]\n",
      "epoch=1,  mn=1600,  batch_mask=[ 8750 44069 21346 41444 13049  7098 33595 37031 35405 24212]\n",
      "epoch=1,  mn=1700,  batch_mask=[16591 17081 54656 28521 38856 38547 25129 37002 48051 23800]\n",
      "epoch=1,  mn=1800,  batch_mask=[14089 53014 42000 20881 12203 54673 50025 59420  5560 29677]\n",
      "\n",
      "epoch=2,  mn=0,  batch_mask=[22462 52927  2447 13086 12674   250  5060 19831 58731 40233]\n",
      "epoch=2,  mn=100,  batch_mask=[17860 31618 51720  7757 10901 50670 17442 48241 31252 45943]\n",
      "epoch=2,  mn=200,  batch_mask=[47442 54383  1958 55955  2411 56631 37167 28642 14934 30425]\n",
      "epoch=2,  mn=300,  batch_mask=[ 3653 47089 28701 54822 56171 23293 50909 32344 28785 20902]\n",
      "epoch=2,  mn=400,  batch_mask=[29043 52011  6285 55930 55129  8455 36326  2841 28006 35843]\n",
      "epoch=2,  mn=500,  batch_mask=[15849 36816 46391 55792 21189 33841 37499  1431 48485 20010]\n",
      "epoch=2,  mn=600,  batch_mask=[27884  9261 59568 12212 16693  2172  5216 41812 23553 38865]\n",
      "epoch=2,  mn=700,  batch_mask=[ 4614 25585 58918 57115 51323 38319 45045 34012 37022 20002]\n",
      "epoch=2,  mn=800,  batch_mask=[53058 19206 50791  6395 12361 39009 19961 18406 57143 44238]\n",
      "epoch=2,  mn=900,  batch_mask=[55379  4163 22760 32385 52744  2680 22618 26178 52131  6634]\n",
      "epoch=2,  mn=1000,  batch_mask=[14922 48391 12333  6810 38410 16097  2576 29655  9765 12995]\n",
      "epoch=2,  mn=1100,  batch_mask=[10632 26983 10286  8962 36470 51283 31002 53616 51584 56442]\n",
      "epoch=2,  mn=1200,  batch_mask=[33846 39313 37619 33745 50166  1478  4640 39088 47351 48322]\n",
      "epoch=2,  mn=1300,  batch_mask=[42775  3603 29121 39568 22387 21001 54176 55558 16349 36275]\n",
      "epoch=2,  mn=1400,  batch_mask=[19186 27548 21858  4878 10774  3050  7821 57512 25828  2089]\n",
      "epoch=2,  mn=1500,  batch_mask=[20653   167 37614 40729 12973 31219 20076 14270 34714 45222]\n",
      "epoch=2,  mn=1600,  batch_mask=[21070 10844  3385 33806 46542 53839 49081 38850 39239 12768]\n",
      "epoch=2,  mn=1700,  batch_mask=[  238 20415 53634  9529 39635 17465 26225  4278 35035 50921]\n",
      "epoch=2,  mn=1800,  batch_mask=[55133 19455 18892  8779 23591 51104 42570 47738 25588 17475]\n",
      "\n",
      "epoch=3,  mn=0,  batch_mask=[46249 59000 32204 40557 32917 45659 25665 47203 54423  7979]\n",
      "epoch=3,  mn=100,  batch_mask=[10355 35427 43187 58425 43896 41308 55466 30350 41886 45839]\n",
      "epoch=3,  mn=200,  batch_mask=[58487 25950 54324 24374 21085  8406 39087 30362 23530  2758]\n",
      "epoch=3,  mn=300,  batch_mask=[25529 13011 27953 50078  6271 49374 34973 46397 29480 40975]\n",
      "epoch=3,  mn=400,  batch_mask=[37244 27338 46526 43977 47780 58990  9611 43714 46962 27474]\n",
      "epoch=3,  mn=500,  batch_mask=[40861 56342 37835 33791 24187 25474 45216 16042  4902 51462]\n",
      "epoch=3,  mn=600,  batch_mask=[29513 35613 25827 32869 15600 36555 25141 42526  6086  7764]\n",
      "epoch=3,  mn=700,  batch_mask=[15396 54409 56234   982 47151 10634  9264 53479 12897 34768]\n",
      "epoch=3,  mn=800,  batch_mask=[ 4939 34391 48859 12657   780 34749 47304 57952  5354 24102]\n",
      "epoch=3,  mn=900,  batch_mask=[38745  2052 23306 33855 19310  2567 22845 14247  7854 36718]\n",
      "epoch=3,  mn=1000,  batch_mask=[39761  7352 48189  4964 22382 49736 30246 37419 21746 12404]\n",
      "epoch=3,  mn=1100,  batch_mask=[ 8087  5116  4369 43396 44462 50200  1082  6849  6462 50081]\n",
      "epoch=3,  mn=1200,  batch_mask=[  315 27161 28824 52714  6425  8097 33369 17084 48256  2120]\n",
      "epoch=3,  mn=1300,  batch_mask=[34389 51635 40846 12948 58025  4593 42890 37490    86 33610]\n",
      "epoch=3,  mn=1400,  batch_mask=[20859 39862 33416 17624 16235 37571 31427 25649 18570  4106]\n",
      "epoch=3,  mn=1500,  batch_mask=[28050 12013 27751 42435 26134 42460 58204 14640 50420  5961]\n",
      "epoch=3,  mn=1600,  batch_mask=[49415 42524 41307 13345 39525 29262  3147 38937 54719 52531]\n",
      "epoch=3,  mn=1700,  batch_mask=[32855 14233 20363 46903  2887 59237 35315  9402 22117  8774]\n",
      "epoch=3,  mn=1800,  batch_mask=[ 9230 26902 51422 54800 47491 19622 23567    94 52977 48768]\n",
      "\n",
      "epoch=4,  mn=0,  batch_mask=[38650 19036 56292 57828 52344 54908 28374 42959 37551 45520]\n",
      "epoch=4,  mn=100,  batch_mask=[53508 44651 15924 42019  7892 18222 13693 25149 22265 53374]\n",
      "epoch=4,  mn=200,  batch_mask=[49355 48257 31112 22735  8791 43772 19585 47582 36892 45163]\n",
      "epoch=4,  mn=300,  batch_mask=[25575 43024 58236 46659 52032  9896 14947  4372 31895 43542]\n",
      "epoch=4,  mn=400,  batch_mask=[  198 49370  4730 26950 27463 20016  2417  3841 11124  5155]\n",
      "epoch=4,  mn=500,  batch_mask=[55366  2367 56484 49002  3483 41359 57538 29504 37195 29944]\n",
      "epoch=4,  mn=600,  batch_mask=[15803 30052 36883 18813 21186 25170 47101 12230 56939  5597]\n",
      "epoch=4,  mn=700,  batch_mask=[44235 23496 12012 33008   541 46361 34660 47801 51453 35847]\n",
      "epoch=4,  mn=800,  batch_mask=[29764  8221  9276 10987 25419 11976 23314 52310 31258 54246]\n",
      "epoch=4,  mn=900,  batch_mask=[39975 51368 23292 22312 23442 16703 29715 14288 55418 24685]\n",
      "epoch=4,  mn=1000,  batch_mask=[31837 42214 37171 17701 21733 40407 15540 43150 27880  4200]\n",
      "epoch=4,  mn=1100,  batch_mask=[45221 40638 37107 47504  7301  7770 58331  4189  7453 12370]\n",
      "epoch=4,  mn=1200,  batch_mask=[54696 36144 45247 32048 40791 35611 48310 18900  9359 40747]\n",
      "epoch=4,  mn=1300,  batch_mask=[17766 16122 58991 31845 40838 35508 32645 14700 31994 16058]\n",
      "epoch=4,  mn=1400,  batch_mask=[25876  2678  8765  3496 44117 57797  3764 32386 13381  3054]\n",
      "epoch=4,  mn=1500,  batch_mask=[49607 56625 27514 49981 53109 49702 30277 39859 56562 11702]\n",
      "epoch=4,  mn=1600,  batch_mask=[ 1601 15621  5694 47131  2152 51783 12899 41051 10327 20276]\n",
      "epoch=4,  mn=1700,  batch_mask=[ 7597  5039  8271 49364 15248  4867 52235  5403 16467  5993]\n",
      "epoch=4,  mn=1800,  batch_mask=[56846 33646 56726 36637 56768  2351  8310 28146 49390 31165]\n",
      "\n",
      "epoch=5,  mn=0,  batch_mask=[36289 54284 35184  1689 39021 13599 50857 22917 30767  7741]\n",
      "epoch=5,  mn=100,  batch_mask=[21000 25448 36436 41754  9866 35394 44742 26504 33611 57450]\n",
      "epoch=5,  mn=200,  batch_mask=[12534 56687 33827 45300 17719 22873 11454 29035 51267  5144]\n",
      "epoch=5,  mn=300,  batch_mask=[38880 38264 35285 34423  6307 54718 34531  5182 18737 35662]\n",
      "epoch=5,  mn=400,  batch_mask=[14915 44940  6312 52348 52989 59100 56640 59576 30323  8089]\n",
      "epoch=5,  mn=500,  batch_mask=[43762 19567 48767  8256 18240 15772 26858 21572  2695 23138]\n",
      "epoch=5,  mn=600,  batch_mask=[19758 48080 15570 17725 45992 53465  3255 46961 29254 27728]\n",
      "epoch=5,  mn=700,  batch_mask=[36628 44361  9933 56118  2218 36062 40126 23622  3974  9102]\n",
      "epoch=5,  mn=800,  batch_mask=[20660 47632 44658 15913 33174 31153  7859 15626 19549 26883]\n",
      "epoch=5,  mn=900,  batch_mask=[15912 40720 10526 22223  5888 30356 34299 12994 32842 35114]\n",
      "epoch=5,  mn=1000,  batch_mask=[45069 13892 31803 22828 23853  6194 28057 43799 12256 26434]\n",
      "epoch=5,  mn=1100,  batch_mask=[25177 56950  1437 19764 27064 40000 59979 42819 45448  9712]\n",
      "epoch=5,  mn=1200,  batch_mask=[57057  8630 52105 35377 15783 23041 40865 28256 27033 59008]\n",
      "epoch=5,  mn=1300,  batch_mask=[19197 32582 54189 55754  8832 55760 15634 28618 51186 41188]\n",
      "epoch=5,  mn=1400,  batch_mask=[31847  5711 30133 50258  2769 53261 28812 54849 54082  4429]\n",
      "epoch=5,  mn=1500,  batch_mask=[17743 15122 52632 10830 44616 23772  2348    42 46453 29088]\n",
      "epoch=5,  mn=1600,  batch_mask=[46198 18782 41186 39444 34354 42040  3421 28938 57143 45636]\n",
      "epoch=5,  mn=1700,  batch_mask=[17163 48938 28426 52935 36223 34981 28668 36074   711 22755]\n",
      "epoch=5,  mn=1800,  batch_mask=[10110 54657 37049 32201 11158 15185 54437 13548 31183 44404]\n",
      "\n",
      "epoch=6,  mn=0,  batch_mask=[44623 44819   790   772   800 58628 29750 38346  5907 27933]\n",
      "epoch=6,  mn=100,  batch_mask=[23927  1041 41512 24563 52922 23111 29837  8804 52228 46043]\n",
      "epoch=6,  mn=200,  batch_mask=[44313 26651 40194 37508 22381 47753 16437 19213 17562 26678]\n",
      "epoch=6,  mn=300,  batch_mask=[ 8175 27273 39352 18510  2377 18173 40753 28842  4395 14948]\n",
      "epoch=6,  mn=400,  batch_mask=[52233 55291  5696    55 26302 14829 15176 43719 30706 31469]\n",
      "epoch=6,  mn=500,  batch_mask=[13267 25135 31667 33252 19797 16490 52586 37089  8114  7865]\n",
      "epoch=6,  mn=600,  batch_mask=[45975 21222 39933 52344 24461 46973 55776 38173  5322  4557]\n",
      "epoch=6,  mn=700,  batch_mask=[38600 56194  8577 39218 39968 39346 37636 20958  4050 41635]\n",
      "epoch=6,  mn=800,  batch_mask=[30493 37303 22575 55784 28342 24398 16256 11133 43312 14894]\n",
      "epoch=6,  mn=900,  batch_mask=[59898 21107  9181 18601 33222 54848  8737 17184 27796 51540]\n",
      "epoch=6,  mn=1000,  batch_mask=[39086 47309 50512 44586  4213  5510  8676 55752  3068 35040]\n",
      "epoch=6,  mn=1100,  batch_mask=[ 1355 55292 23920   906 11800 19097 10006 20170 12390 38549]\n",
      "epoch=6,  mn=1200,  batch_mask=[36412 18933 55885 22516 18110   982  4741  8983 53259 52901]\n",
      "epoch=6,  mn=1300,  batch_mask=[59334  9009  8138 11407 12338  4719 46584 25237  7680  4722]\n",
      "epoch=6,  mn=1400,  batch_mask=[10488  7144 48675 20430 44982 55489 52931 11174 51186 59479]\n",
      "epoch=6,  mn=1500,  batch_mask=[29416 18780 59125 17382   322  7320 39467 43190 11088 33549]\n",
      "epoch=6,  mn=1600,  batch_mask=[ 1092 56059 49017 58076 41875 55436 14407 10118 20107  4207]\n",
      "epoch=6,  mn=1700,  batch_mask=[28282 59382  5094 51224 34440 14141 15312 59849 34295 25362]\n",
      "epoch=6,  mn=1800,  batch_mask=[20437  7291 16330 58336 29374 45114 49835 20695 55500 59029]\n",
      "\n",
      "epoch=7,  mn=0,  batch_mask=[26905 33388 14278 24337 55472 34076 32413 24967 39001 53792]\n",
      "epoch=7,  mn=100,  batch_mask=[20884  4081 55128 49423 15229 22766 14440 36619 38557 54144]\n",
      "epoch=7,  mn=200,  batch_mask=[ 2900 15483  4641   815 22445 52941 10170 39723 19380  1073]\n",
      "epoch=7,  mn=300,  batch_mask=[55035 39655 45585 44959 17392 31680 41706 23598 55004 52770]\n",
      "epoch=7,  mn=400,  batch_mask=[11566 35102 29492 41740 58136 50540 32919 48931 17398 23622]\n",
      "epoch=7,  mn=500,  batch_mask=[22593 30054  7150 30230 26032 38311 32857  4319 29357  6729]\n",
      "epoch=7,  mn=600,  batch_mask=[ 6149 18322 35233 40759 39932  1176 23162 25527 40131 58157]\n",
      "epoch=7,  mn=700,  batch_mask=[ 7576 12149 54589 58194 19771 49392 18660 23166 36265 47934]\n",
      "epoch=7,  mn=800,  batch_mask=[44584 23520 23362 40325 30388 58762  5500 58614 37592 12545]\n",
      "epoch=7,  mn=900,  batch_mask=[51778 52114  8465 34030 16590 10766 37273  4441 39598 22183]\n",
      "epoch=7,  mn=1000,  batch_mask=[ 6025 39249 13185 56993 48566 16275 30543 17721 28388 40398]\n",
      "epoch=7,  mn=1100,  batch_mask=[55419  1021 24090 57508 40710 37197 24477 53712 53274  1257]\n",
      "epoch=7,  mn=1200,  batch_mask=[20222 42629 36113 55186 54293 53331 14527  7552 43930 25144]\n",
      "epoch=7,  mn=1300,  batch_mask=[35796 32538 20334 51965 14857  6965 40839 20187 21494 24524]\n",
      "epoch=7,  mn=1400,  batch_mask=[52001 39077 47442  4292 51477  7075  2786 42197  5126 51133]\n",
      "epoch=7,  mn=1500,  batch_mask=[42646 56106 41501  6619 29225 20309  6074 26515 34225   516]\n",
      "epoch=7,  mn=1600,  batch_mask=[18732 56692 18454 26877 59442 52122 14421 38159 30993  3794]\n",
      "epoch=7,  mn=1700,  batch_mask=[54354  3848 44247 20648 32717  6379 54832 21060 45116  8960]\n",
      "epoch=7,  mn=1800,  batch_mask=[50301  6920 20817 18609 24859 59150 35935  4142 54278 50138]\n",
      "\n",
      "epoch=8,  mn=0,  batch_mask=[55523 50856  1522 38904 10759 30268 21374  6433   747 10387]\n",
      "epoch=8,  mn=100,  batch_mask=[22632 24391 44103 45961 44946 20903 12449  1441 46622  2904]\n",
      "epoch=8,  mn=200,  batch_mask=[43427 46356 54457 20049  2887 22382 47817 19943 11662 49264]\n",
      "epoch=8,  mn=300,  batch_mask=[ 9157 44429 49432 19527 47375 10275 22400 53890  9564 30656]\n",
      "epoch=8,  mn=400,  batch_mask=[59744 30822 56126 21062 34236  9469  5071 35832 51300 36667]\n",
      "epoch=8,  mn=500,  batch_mask=[59658  2919 35149  9722 59340 13295 34850  6263 26020  4024]\n",
      "epoch=8,  mn=600,  batch_mask=[48750 17143 40665 25391  3835 15345 42317 21742  7109  3629]\n",
      "epoch=8,  mn=700,  batch_mask=[36225  6281 47730 33653 28948 58723 25730  6946 11227 40932]\n",
      "epoch=8,  mn=800,  batch_mask=[23634 28539 54817 55893 51917 32568 20910 35565 47449  7704]\n",
      "epoch=8,  mn=900,  batch_mask=[45291  8186 27149 39201 18774 51824 23573 42820    79 40350]\n",
      "epoch=8,  mn=1000,  batch_mask=[ 4705 26502 32615 45472 40616 44354 17788 43097 51127  3301]\n",
      "epoch=8,  mn=1100,  batch_mask=[54431 50635 10580   564 16288  2828 43098 33012  3030 42322]\n",
      "epoch=8,  mn=1200,  batch_mask=[24731 41877 20386 25523 28971 30719 48563  2903 28702 29173]\n",
      "epoch=8,  mn=1300,  batch_mask=[ 6537 57201 34629 23569 30518 10063  8528 18492  2628 15890]\n",
      "epoch=8,  mn=1400,  batch_mask=[29026  9625 54237 28811 23091 59833  5039 38430 30859 14363]\n",
      "epoch=8,  mn=1500,  batch_mask=[25169 30225 19534 18255 45352 13959 48320  7321 40234 23154]\n",
      "epoch=8,  mn=1600,  batch_mask=[19921 16260  4098 31908 59240  8191 28861 39032 38276 43657]\n",
      "epoch=8,  mn=1700,  batch_mask=[ 1073 26793 53157  3979 14256 40460  5652 42419 12487 32502]\n",
      "epoch=8,  mn=1800,  batch_mask=[31003 28783 52808  1015 19125 13285 46908  7049 11977 41051]\n",
      "\n",
      "epoch=9,  mn=0,  batch_mask=[26874  4740 35959 40144 45538  8079 44187 12101 10136 32022]\n",
      "epoch=9,  mn=100,  batch_mask=[40935 10296 26945  6466 46267 34349 31726 26602 39572 14602]\n",
      "epoch=9,  mn=200,  batch_mask=[39476 50907 47008  6649  4203 12022 53339 31010 54967 32292]\n",
      "epoch=9,  mn=300,  batch_mask=[31283  3004 46167 20693 40694 38790 30637 37921  9940 17013]\n",
      "epoch=9,  mn=400,  batch_mask=[18821 11518 54394 26487  8557 21884 51866 31772  4244 12985]\n",
      "epoch=9,  mn=500,  batch_mask=[26279 53971   450 44816 50709 57537 11598 22335 47284 19785]\n",
      "epoch=9,  mn=600,  batch_mask=[16883 49902  4566 41024 40778 37509 21822 17813 59127 11568]\n",
      "epoch=9,  mn=700,  batch_mask=[39796 52537  5376 23145  3799 37378 17461 23853 44407 53219]\n",
      "epoch=9,  mn=800,  batch_mask=[ 7630 34148 11117 37856 56049 36472 54294 29749 49696 38498]\n",
      "epoch=9,  mn=900,  batch_mask=[22645 34210 34986 58550 56878  4512  4006  7515 41937 17872]\n",
      "epoch=9,  mn=1000,  batch_mask=[36028 56092 19132 42043 27247 11715 18034 50138  7628 50494]\n",
      "epoch=9,  mn=1100,  batch_mask=[48904 22013 34573  2967 54026 41797  4596 30051 59466 58085]\n",
      "epoch=9,  mn=1200,  batch_mask=[55665 59620 16185 47958 40509 13713 21326 16856  3944 18172]\n",
      "epoch=9,  mn=1300,  batch_mask=[15439 13199 31480 54998  6219 13773 16042  5334 42409 55210]\n",
      "epoch=9,  mn=1400,  batch_mask=[28337 24614  7170  6496  5324 25491 34578 17946 16386 16915]\n",
      "epoch=9,  mn=1500,  batch_mask=[21557 24588 47620 21186 56106 45715 59648 31450  1635 29250]\n",
      "epoch=9,  mn=1600,  batch_mask=[37828 17708  1309 15871 45724 40159 49546 11053 31560 13480]\n",
      "epoch=9,  mn=1700,  batch_mask=[57217 57350 18633  5973 58096 39724 35580 52253 29070 17442]\n",
      "epoch=9,  mn=1800,  batch_mask=[58022 59239 52301 27193  9001  1030 41896 26763   669 30673]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "train_size = train_labels.shape[0]\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "network = None #ダミー\n",
    "minibatch_num = np.ceil(train_size/batch_size).astype(int) # ミニバッチの個数\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    \n",
    "    # indexを定義し、シャッフルする\n",
    "    index = np.arange(train_size)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    for mn in range(minibatch_num):\n",
    "        \"\"\"\n",
    "        非復元抽出によるループ\n",
    "        \"\"\"\n",
    "        batch_mask = index[batch_size * mn : batch_size * (mn + 1)]\n",
    "        if np.mod(mn, 100)==0:\n",
    "            print(\"epoch=%s, \"%epoch,\"mn=%s, \"%mn , \"batch_mask=%s\"%batch_mask[:10])\n",
    "        x_batch = train[batch_mask]\n",
    "        y_batch = train_labels[batch_mask]\n",
    "\n",
    "        trainer(network, x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
