{"cells":[{"cell_type":"markdown","metadata":{"id":"F_d6SnbA69H7"},"source":["# 系列変換モデルで学習する"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","try:\n","    from google.colab import files\n","    print('Google Colab. 上での実行です')\n","    print('「ファイルを選択」から、notebook/dataset フォルダの中身をすべてを選択し、アップロードしてください')\n","    print('===========')\n","    files.upload()\n","    !mkdir dataset\n","    !mv * ./dataset\n","    print('次に notebook/common フォルダの中身をすべてを選択し、アップロードしてください')\n","    print('===========')\n","    files.upload()\n","    !mkdir common\n","    !mv *.py ./common\n","except:\n","    print('ローカル環境での実行です')\n","\n","\n","from dataset import sequence\n","from common.optimizer import Adam\n","from common.trainer import Trainer\n","from common.util import eval_seq2seq\n","from common.seq2seq import Seq2seq # seq2seq\n","from common.attention_seq2seq import AttentionSeq2seq # アテンション付きseq2seq\n","from common.attention_biseq2seq import AttentionBiSeq2seq # エンコーダ側LSTMが双方向になったアテンション付きseq2seq"],"metadata":{"id":"Rxwopnc26_3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6bNJzwM69IC"},"outputs":[],"source":["# データの読み込み\n","(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n","char_to_id, id_to_char = sequence.get_vocab()\n","\n","# ハイパーパラメータの設定\n","vocab_size = len(char_to_id)\n","wordvec_size = 16\n","hidden_size = 256\n","batch_size = 256\n","max_epoch = 20 # 計算時間削減のため20epochとしているが，十分な学習を行うには更に増やす必要がある"]},{"cell_type":"markdown","metadata":{"id":"pGMnQDcj69ID"},"source":["### モデルの選択\n","モデルを切り替えて、結果を比較してみましょう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSJGiSLB69IE"},"outputs":[],"source":["# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n","# model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n","model = AttentionBiSeq2seq(vocab_size, wordvec_size, hidden_size)"]},{"cell_type":"markdown","metadata":{"id":"DkoSBCHP69IF"},"source":["### 学習"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"jc2nndqV69IF"},"outputs":[],"source":["# 最適化手法の設定\n","optimizer = Adam()\n","\n","# 学習のオブジェクトを生成\n","trainer = Trainer(model, optimizer)\n","\n","# 学習のループ\n","acc_list = []\n","loss_list = []\n","for epoch in range(max_epoch):\n","    \n","    # trainデータで1epoch分の計算\n","    trainer.fit(x_train, t_train, max_epoch=1,batch_size=batch_size)\n","\n","    # testデータで精度を確認する\n","    correct_num = 0\n","    for i in range(len(x_test)):\n","        question, correct = x_test[[i]], t_test[[i]]\n","        verbose = i < 10\n","        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose) \n","\n","    # 精度算出\n","    acc = float(correct_num) / len(x_test)\n","    acc_list.append(acc)\n","    print('val acc %.3f%%' % (acc * 100))\n","\n","    # loss算出\n","    loss = model.forward(x_test, t_test)\n","    loss_list.append(loss)\n","    print('val loss %.3f' % (loss))\n","    \n","    # 重み保存\n","    model.save_params()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0zeZtlF69IH"},"outputs":[],"source":["# Accuracyの描画\n","x = np.arange(len(acc_list))\n","plt.plot(x, acc_list, marker='o')\n","plt.xlabel('epochs')\n","plt.ylabel('accuracy')\n","plt.ylim(-0.05, 1.05)\n","plt.show()\n","\n","# Lossの描画\n","plt.plot(x, loss_list, marker='o')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","# plt.ylim(-0.05, 0.1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mNzG0PKD69II"},"source":["### [演習]\n","* Seq2seq、AttentionSeq2seq、AttentionBiSeq2seqのそれぞれの場合を計算し、結果を比較してみましょう。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"7_8_train_with_seq-to-seq.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}